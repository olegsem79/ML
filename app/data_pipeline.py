# data_pipeline.py
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils.class_weight import compute_class_weight
import os

def setup_tensorflow():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ TensorFlow –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è TensorFlow
    tf.config.optimizer.set_jit(True)  # –í–∫–ª—é—á–µ–Ω–∏–µ XLA-–∫–æ–º–ø–∏–ª—è—Ü–∏–∏
    tf.config.threading.set_intra_op_parallelism_threads(8)
    
    # –î–ª—è GPU
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            print(e)
    
    # –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ warnings
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    tf.get_logger().setLevel('ERROR')

def create_data_pipeline(
    image_path, 
    height=224, 
    width=224, 
    batch_size=32, 
    validation_split=0.2,
    seed=42
):
    """
    –°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    
    Args:
        image_path (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        height (int): –í—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        width (int): –®–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π  
        batch_size (int): –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        validation_split (float): –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        seed (int): Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Returns:
        tuple: (train_data, validation_data, train_data_raw, class_names, num_classes)
    """
    print("üîÑ –°–û–ó–î–ê–ï–ú –ü–ê–ô–ü–õ–ê–ô–ù...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_data_raw = tf.keras.preprocessing.image_dataset_from_directory(
        image_path,
        validation_split=validation_split,
        subset='training',
        seed=seed,
        image_size=(height, width),
        batch_size=batch_size,
        shuffle=True
    )

    validation_data_raw = tf.keras.preprocessing.image_dataset_from_directory(
        image_path,
        validation_split=validation_split,
        subset='validation', 
        seed=seed,
        image_size=(height, width),
        batch_size=batch_size,
        shuffle=True
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
    images, labels = next(iter(train_data_raw))
    print(f"Labels shape: {labels.shape}")
    print(f"Labels: {labels.numpy()[:10]}")

    class_names = train_data_raw.class_names
    num_classes = len(class_names)
    print(f"üè∑Ô∏è –ö–ª–∞—Å—Å—ã: {class_names}")
    
    # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
    data_augmentation = tf.keras.Sequential([
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.2),
        tf.keras.layers.RandomZoom(0.2),
        tf.keras.layers.RandomContrast(0.2),
        tf.keras.layers.RandomBrightness(0.2),
    ])

    def preprocess_and_augment(image, label, training=False):
        """–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è"""
        image = tf.cast(image, tf.float32)
        if training:
            image = data_augmentation(image, training=True)
        return image, label

    # –§–∏–Ω–∞–ª—å–Ω—ã–µ datasets
    train_data = (
        train_data_raw
        .map(lambda x, y: preprocess_and_augment(x, y, training=True),
             num_parallel_calls=tf.data.AUTOTUNE)
        .cache()
        .prefetch(tf.data.AUTOTUNE)
    )

    validation_data = (
        validation_data_raw
        .map(lambda x, y: preprocess_and_augment(x, y, training=False), 
             num_parallel_calls=tf.data.AUTOTUNE)
        .cache()
        .prefetch(tf.data.AUTOTUNE)
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞:")
    for images, labels in train_data.take(1):
        print(f"Shape: {images.shape}")
        print(f"Data range: {tf.reduce_min(images):.3f} to {tf.reduce_max(images):.3f}")
        print(f"Mean: {tf.reduce_mean(images):.3f}")
        break

    print("‚úÖ –ü–∞–π–ø–ª–∞–π–Ω —Å–æ–∑–¥–∞–Ω!")
    
    return train_data, validation_data, train_data_raw, class_names, num_classes

def get_class_weights_from_dataset(dataset, class_names):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ tf.data.Dataset
    
    Args:
        dataset: tf.data.Dataset —Å –º–µ—Ç–∫–∞–º–∏
        class_names: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤
    
    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ {class_index: weight}
    """
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–µ—Ç–∫–∏
    all_labels = []
    for images, labels in dataset:
        all_labels.extend(labels.numpy())
    
    y_train = np.array(all_labels)
    
    # –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤
    class_weights = compute_class_weight(
        'balanced',
        classes=np.unique(y_train),
        y=y_train
    )
    
    weights_dict = dict(enumerate(class_weights))
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(12, 5))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    plt.subplot(1, 2, 1)
    unique, counts = np.unique(y_train, return_counts=True)
    bars = plt.bar(range(len(unique)), counts)
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤')
    plt.xlabel('–ö–ª–∞—Å—Å')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
    plt.xticks(range(len(unique)), [class_names[i] for i in unique], rotation=45)
    
    for bar, count in zip(bars, counts):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, 
                str(count), ha='center', va='bottom')
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    plt.subplot(1, 2, 2)
    bars = plt.bar(weights_dict.keys(), weights_dict.values())
    plt.title('–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤')
    plt.xlabel('–ö–ª–∞—Å—Å')
    plt.ylabel('–í–µ—Å')
    plt.xticks(range(len(unique)), [class_names[i] for i in unique], rotation=45)
    
    for bar, weight in zip(bars, weights_dict.values()):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                f'{weight:.2f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()
    
    # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    print("üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:")
    print("-" * 50)
    for i, class_name in enumerate(class_names):
        count = counts[i] if i < len(counts) else 0
        weight = weights_dict.get(i, 0)
        print(f"{class_name:15} | {count:4} | –≤–µ—Å: {weight:.2f}")
    
    return weights_dict

def show_augmented_batch(train_data, class_names):
    """
    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ train_data
    
    Args:
        train_data: tf.data.Dataset —Å –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        class_names: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤
    """
    print("üé≤ –ê–£–ì–ú–ï–ù–¢–ò–†–û–í–ê–ù–ù–´–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø –ò–ó TRAIN_DATA:")
    
    # –ë–µ—Ä–µ–º –æ–¥–∏–Ω –±–∞—Ç—á –∏–∑ train_data
    for images, labels in train_data.take(1):
        print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {images.shape}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
        
        plt.figure(figsize=(15, 10))
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 8 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –±–∞—Ç—á–∞
        for i in range(min(8, len(images))):
            plt.subplot(2, 4, i+1)
            
            # –ë–µ—Ä–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –±–∞—Ç—á–∞
            image = images[i]
            label = labels[i]
            
            # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (–∏–∑ [0,1] –≤ [0,255])
            if tf.reduce_max(image) <= 1.0:
                image = image * 255
            image = tf.cast(image, tf.uint8)
            
            plt.imshow(image)
            plt.title(f'Class: {class_names[label.numpy()]}')
            plt.axis('off')
        
        plt.suptitle('–ê–£–ì–ú–ï–ù–¢–ò–†–û–í–ê–ù–ù–´–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø –ò–ó TRAIN_DATA', fontsize=16)
        plt.tight_layout()
        plt.show()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞–Ω–Ω—ã—Ö
        print(f"–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞—Ç—á–µ: {tf.reduce_min(images):.3f} –¥–æ {tf.reduce_max(images):.3f}")
        break