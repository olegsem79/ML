{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b430d3f8-24a4-45fa-9cec-907a5708a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oleg/projects/ML/OWL-ViT\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe5786-40e4-44cd-bbd4-02b56821bb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2c808-e7a6-4094-a715-ed589048ea09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a046b-4c58-451d-a9ca-930242661111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1714509b-9d8c-4347-91f4-11a66062c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 3c237a92b724ac939f84493963c00952.jpg -> /home/oleg/projects/ML/OWL-ViT/processed/3c237a92b724ac939f84493963c00952.json\n",
      "‚úÖ gettyimages-524401415-1024x1024.jpg -> /home/oleg/projects/ML/OWL-ViT/processed/gettyimages-524401415-1024x1024.json\n",
      "‚úÖ 05a992e228da2abf2d571db367a3febf.jpg -> /home/oleg/projects/ML/OWL-ViT/processed/05a992e228da2abf2d571db367a3febf.json\n",
      "‚úÖ elegant-portrait-black-cat-indoors_23-2151890796.jpg -> /home/oleg/projects/ML/OWL-ViT/processed/elegant-portrait-black-cat-indoors_23-2151890796.json\n",
      "‚úÖ cat-pictures-zc3gu0636kmldm04.jpg -> /home/oleg/projects/ML/OWL-ViT/processed/cat-pictures-zc3gu0636kmldm04.json\n",
      "‚úÖ 409052_size1.jpg -> /home/oleg/projects/ML/OWL-ViT/processed/409052_size1.json\n",
      "‚úÖ 1e64d77677f5ce8714d552f30f2422bb.jpg -> /home/oleg/projects/ML/OWL-ViT/processed/1e64d77677f5ce8714d552f30f2422bb.json\n",
      "‚úÖ 0d354ad89e92986b19b10a8ac2797dfb.jpg -> /home/oleg/projects/ML/OWL-ViT/processed/0d354ad89e92986b19b10a8ac2797dfb.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –ø–∞–ø–∫–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def quick_batch_process(input_folder, output_folder, what_to_find):\n",
    "    \"\"\"–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\"\"\"\n",
    "    \n",
    "    from pathlib import Path\n",
    "    Path(output_folder).mkdir(exist_ok=True)\n",
    "    \n",
    "    processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "    model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "    \n",
    "    for image_file in os.listdir(input_folder):\n",
    "        if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            input_path = os.path.join(input_folder, image_file)\n",
    "            base_name = os.path.splitext(image_file)[0]\n",
    "            \n",
    "            # –û–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "            image = Image.open(input_path)\n",
    "            texts = [[what_to_find]]\n",
    "            inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "            target_sizes = torch.Tensor([image.size[::-1]])\n",
    "            results = processor.post_process_object_detection(\n",
    "                outputs=outputs, target_sizes=target_sizes, threshold=0.2\n",
    "            )\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON\n",
    "            output_json = {\n",
    "                \"file\": image_file,\n",
    "                \"objects_found\": len([s for s in results[0][\"scores\"] if s > 0.2]),\n",
    "                \"prompt\": what_to_find\n",
    "            }\n",
    "            \n",
    "            json_path = os.path.join(output_folder, f\"{base_name}.json\")\n",
    "            with open(json_path, 'w') as f:\n",
    "                json.dump(output_json, f, indent=2)\n",
    "            \n",
    "            print(f\"‚úÖ {image_file} -> {json_path}\")\n",
    "\n",
    "# –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫\n",
    "quick_batch_process(\n",
    "    \"/home/oleg/projects/ML/OWL-ViT/images\",\n",
    "    \"/home/oleg/projects/ML/OWL-ViT/processed\", \n",
    "    \"a person, a cat, a car\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64c884-70c2-45b0-9d30-435be92943c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8593e2e-b870-4a2e-96de-6aab3a38f01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 18:37:18.669768: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-31 18:37:18.711214: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-31 18:37:20.366316: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ó–ê–ü–£–°–ö –ü–ê–ö–ï–¢–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò...\n",
      "üìÅ –í—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: /home/oleg/projects/ML/OWL-ViT/images\n",
      "üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: /home/oleg/projects/ML/OWL-ViT/processed\n",
      "üîç –ü–æ–∏—Å–∫ –æ–±—ä–µ–∫—Ç–æ–≤: a person, a cat, a car\n",
      "============================================================\n",
      "ü¶â –ó–∞–≥—Ä—É–∂–∞–µ–º Owl-ViT...\n",
      "üìÅ –ù–∞–π–¥–µ–Ω–æ 8 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ /home/oleg/projects/ML/OWL-ViT/images\n",
      "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: 3c237a92b724ac939f84493963c00952.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oleg/miniconda3/envs/tensflow/lib/python3.11/site-packages/transformers/models/owlvit/processing_owlvit.py:217: FutureWarning: `post_process_object_detection` method is deprecated for OwlVitProcessor and will be removed in v5. Use `post_process_grounded_object_detection` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ì–æ—Ç–æ–≤–æ: 3c237a92b724ac939f84493963c00952.jpg\n",
      "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: gettyimages-524401415-1024x1024.jpg\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ: gettyimages-524401415-1024x1024.jpg\n",
      "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: 05a992e228da2abf2d571db367a3febf.jpg\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ: 05a992e228da2abf2d571db367a3febf.jpg\n",
      "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: elegant-portrait-black-cat-indoors_23-2151890796.jpg\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ: elegant-portrait-black-cat-indoors_23-2151890796.jpg\n",
      "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: cat-pictures-zc3gu0636kmldm04.jpg\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ: cat-pictures-zc3gu0636kmldm04.jpg\n",
      "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: 409052_size1.jpg\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ: 409052_size1.jpg\n",
      "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: 1e64d77677f5ce8714d552f30f2422bb.jpg\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ: 1e64d77677f5ce8714d552f30f2422bb.jpg\n",
      "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: 0d354ad89e92986b19b10a8ac2797dfb.jpg\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ: 0d354ad89e92986b19b10a8ac2797dfb.jpg\n",
      "üéâ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!\n",
      "üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: /home/oleg/projects/ML/OWL-ViT/processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def process_folder_with_owlvit(input_folder, output_folder, text_prompts):\n",
    "    \"\"\"\n",
    "    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞–ø–∫–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    \n",
    "    Args:\n",
    "        input_folder: –ø–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "        output_folder: –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        text_prompts: —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é\n",
    "    \"\"\"\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ—Ç\n",
    "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "    print(\"ü¶â –ó–∞–≥—Ä—É–∂–∞–µ–º Owl-ViT...\")\n",
    "    processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "    model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "    image_files = [f for f in os.listdir(input_folder) \n",
    "                  if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "    \n",
    "    print(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {input_folder}\")\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        try:\n",
    "            input_path = os.path.join(input_folder, image_file)\n",
    "            base_name = os.path.splitext(image_file)[0]\n",
    "            \n",
    "            print(f\"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {image_file}\")\n",
    "            \n",
    "            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "            result = process_single_image(\n",
    "                input_path, text_prompts, processor, model\n",
    "            )\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "            save_results(output_folder, base_name, result, input_path)\n",
    "            \n",
    "            print(f\"‚úÖ –ì–æ—Ç–æ–≤–æ: {image_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ —Å {image_file}: {e}\")\n",
    "\n",
    "def process_single_image(image_path, text_prompts, processor, model):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\"\"\"\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã\n",
    "    texts = [text_prompts.split(\",\")]\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–¥–µ–ª—å—é\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏\n",
    "    target_sizes = torch.Tensor([image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs, target_sizes=target_sizes, threshold=0.1\n",
    "    )\n",
    "    \n",
    "    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    detections = []\n",
    "    for box, score, label in zip(results[0][\"boxes\"], results[0][\"scores\"], results[0][\"labels\"]):\n",
    "        if score > 0.2:  # –§–∏–ª—å—Ç—Ä –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            detection = {\n",
    "                \"label\": texts[0][label],\n",
    "                \"confidence\": float(score),\n",
    "                \"bbox\": [x1, y1, x2, y2],\n",
    "                \"bbox_normalized\": [  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (0-1)\n",
    "                    x1 / image.width, \n",
    "                    y1 / image.height, \n",
    "                    x2 / image.width, \n",
    "                    y2 / image.height\n",
    "                ]\n",
    "            }\n",
    "            detections.append(detection)\n",
    "    \n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"image_size\": [image.width, image.height],\n",
    "        \"text_prompts\": text_prompts,\n",
    "        \"detections\": detections,\n",
    "        \"total_detections\": len(detections)\n",
    "    }\n",
    "\n",
    "def save_results(output_folder, base_name, result, original_image_path):\n",
    "    \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\"\"\"\n",
    "    \n",
    "    # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º\n",
    "    json_path = os.path.join(output_folder, f\"{base_name}_detections.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # 2. –°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π\n",
    "    image_with_boxes = create_image_with_boxes(original_image_path, result)\n",
    "    image_output_path = os.path.join(output_folder, f\"{base_name}_annotated.jpg\")\n",
    "    image_with_boxes.save(image_output_path, quality=95)\n",
    "    \n",
    "    # 3. –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç\n",
    "    report_path = os.path.join(output_folder, f\"{base_name}_report.txt\")\n",
    "    create_text_report(report_path, result)\n",
    "\n",
    "def create_image_with_boxes(image_path, result):\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å bounding boxes\"\"\"\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
    "    \n",
    "    for i, detection in enumerate(result['detections']):\n",
    "        color = colors[i % len(colors)]\n",
    "        x1, y1, x2, y2 = detection['bbox']\n",
    "        \n",
    "        # –†–∏—Å—É–µ–º bounding box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "        \n",
    "        # –†–∏—Å—É–µ–º –ø–æ–¥–ø–∏—Å—å\n",
    "        label = f\"{detection['label']} ({detection['confidence']:.2f})\"\n",
    "        draw.rectangle([x1, y1-25, x1+len(label)*8, y1], fill=color)\n",
    "        draw.text((x1+5, y1-20), label, fill='white')\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_text_report(report_path, result):\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç\"\"\"\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(\"–û–¢–ß–ï–¢ –û –î–ï–¢–ï–ö–¶–ò–ò –û–ë–™–ï–ö–¢–û–í\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"üìÅ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {os.path.basename(result['image_path'])}\\n\")\n",
    "        f.write(f\"üìê –†–∞–∑–º–µ—Ä: {result['image_size'][0]}x{result['image_size'][1]}\\n\")\n",
    "        f.write(f\"üîç –ó–∞–ø—Ä–æ—Å: {result['text_prompts']}\\n\")\n",
    "        f.write(f\"üìä –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {result['total_detections']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"üéØ –û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –û–ë–™–ï–ö–¢–´:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        for i, detection in enumerate(result['detections'], 1):\n",
    "            f.write(f\"{i}. {detection['label']}\\n\")\n",
    "            f.write(f\"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {detection['confidence']:.3f}\\n\")\n",
    "            f.write(f\"   BBox: [{detection['bbox'][0]:.1f}, {detection['bbox'][1]:.1f}, \"\n",
    "                   f\"{detection['bbox'][2]:.1f}, {detection['bbox'][3]:.1f}]\\n\")\n",
    "            f.write(f\"   BBox (–Ω–æ—Ä–º.): [{detection['bbox_normalized'][0]:.3f}, \"\n",
    "                   f\"{detection['bbox_normalized'][1]:.3f}, {detection['bbox_normalized'][2]:.3f}, \"\n",
    "                   f\"{detection['bbox_normalized'][3]:.3f}]\\n\\n\")\n",
    "\n",
    "# üéØ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "\n",
    "    \n",
    "    INPUT_FOLDER = \"/home/oleg/projects/ML/OWL-ViT/images\"  # –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "    OUTPUT_FOLDER = \"/home/oleg/projects/ML/OWL-ViT/processed\"  # –ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    TEXT_PROMPTS = \"a person, a cat, a car\"  # –ß—Ç–æ –∏—â–µ–º\n",
    "    \n",
    "    print(\"üöÄ –ó–ê–ü–£–°–ö –ü–ê–ö–ï–¢–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò...\")\n",
    "    print(f\"üìÅ –í—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {INPUT_FOLDER}\")\n",
    "    print(f\"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {OUTPUT_FOLDER}\")\n",
    "    print(f\"üîç –ü–æ–∏—Å–∫ –æ–±—ä–µ–∫—Ç–æ–≤: {TEXT_PROMPTS}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    process_folder_with_owlvit(INPUT_FOLDER, OUTPUT_FOLDER, TEXT_PROMPTS)\n",
    "    \n",
    "    print(\"üéâ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!\")\n",
    "    print(f\"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be6379-0a22-4eb1-9579-b5e069002879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b4d64-0f68-4774-9a07-8df1ed545288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c153d8-ef10-4ff1-a477-567a22fb8b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensflow",
   "language": "python",
   "name": "tensflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
