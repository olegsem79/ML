{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bdc645-2b43-41d7-9404-35803919e577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8adfa524-1b3b-476b-bd84-f27d6acc6011",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'groundingdino'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgroundingdino\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model, load_image, predict\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msam2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msam2_image_predictor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAM2ImagePredictor\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimple_grounding_sam2\u001b[39m(image_path, text_prompt):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'groundingdino'"
     ]
    }
   ],
   "source": [
    "# Установи: pip install groundingdino-py\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "def simple_grounding_sam2(image_path, text_prompt):\n",
    "    \"\"\"Простой GroundingDINO + SAM2\"\"\"\n",
    "    \n",
    "    print(f\"🔍 Ищем: {text_prompt}\")\n",
    "    \n",
    "    # 1. Загружаем модели\n",
    "    print(\"📥 Загружаем GroundingDINO...\")\n",
    "    grounding_model = load_model(\n",
    "        \"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \n",
    "        \"weights/groundingdino_swint_ogc.pth\"\n",
    "    )\n",
    "    \n",
    "    print(\"📥 Загружаем SAM2...\")\n",
    "    sam_model = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-tiny\")\n",
    "    \n",
    "    # 2. Загружаем изображение\n",
    "    image_source, image = load_image(image_path)\n",
    "    \n",
    "    # 3. Ищем объекты по тексту\n",
    "    print(\"🎯 Ищем объекты...\")\n",
    "    boxes, scores, phrases = predict(\n",
    "        model=grounding_model,\n",
    "        image=image,\n",
    "        caption=text_prompt,\n",
    "        box_threshold=0.3,\n",
    "        text_threshold=0.25\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Найдено: {len(boxes)} объектов\")\n",
    "    for i, (phrase, score) in enumerate(zip(phrases, scores)):\n",
    "        print(f\"   {i+1}. {phrase} (уверенность: {score:.3f})\")\n",
    "    \n",
    "    # 4. Сегментируем найденные объекты\n",
    "    print(\"✂️ Сегментируем...\")\n",
    "    sam_model.set_image(image_source)\n",
    "    \n",
    "    all_masks = []\n",
    "    \n",
    "    for i, box in enumerate(boxes):\n",
    "        # Конвертируем координаты\n",
    "        H, W = image_source.shape[1], image_source.shape[2]\n",
    "        x1, y1, x2, y2 = box * torch.tensor([W, H, W, H])\n",
    "        bbox = np.array([x1, y1, x2, y2])\n",
    "        \n",
    "        # Сегментируем объект\n",
    "        masks, mask_scores, _ = sam_model.predict(box=bbox[None, :])\n",
    "        all_masks.append(masks[0])\n",
    "    \n",
    "    # 5. Показываем результат\n",
    "    print(\"🖼️ Показываем результат...\")\n",
    "    \n",
    "    # Конвертируем изображение для отображения\n",
    "    image_display = image_source[0].permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image_display)\n",
    "    \n",
    "    # Рисуем маски\n",
    "    for i, mask in enumerate(all_masks):\n",
    "        # Случайный цвет для каждой маски\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        plt.imshow(mask_image)\n",
    "        \n",
    "        # Подпись объекта\n",
    "        center_y, center_x = np.where(mask)\n",
    "        if len(center_x) > 0 and len(center_y) > 0:\n",
    "            text_x = np.mean(center_x)\n",
    "            text_y = np.mean(center_y)\n",
    "            \n",
    "            plt.text(text_x, text_y, f\"{phrases[i]}\\n({scores[i]:.2f})\", \n",
    "                    fontsize=12, fontweight='bold', color='white',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='black', alpha=0.8),\n",
    "                    ha='center', va='center')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Найдено: {text_prompt}\\n{len(boxes)} объектов\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"🎉 Готово!\")\n",
    "\n",
    "# 🎯 ПРОСТЫЕ ПРИМЕРЫ:\n",
    "\n",
    "print(\"=== ПРИМЕР 1: Люди и техника ===\")\n",
    "simple_grounding_sam2(\n",
    "    \"/home/oleg/projects/ML/CLIP/images/0d354ad89e92986b19b10a8ac2797dfb.jpg\",\n",
    "    \"person . laptop . cup\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== ПРИМЕР 2: Только человек ===\")\n",
    "simple_grounding_sam2(\n",
    "    \"/home/oleg/projects/ML/CLIP/images/0d354ad89e92986b19b10a8ac2797dfb.jpg\",\n",
    "    \"person\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== ПРИМЕР 3: Растения и посуда ===\")\n",
    "simple_grounding_sam2(\n",
    "    \"/home/oleg/projects/ML/CLIP/images/0d354ad89e92986b19b10a8ac2797dfb.jpg\",\n",
    "    \"plant . cup\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec31fe9-b0ae-4c5c-ba47-6e9b6e038616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729b5a1-c1b4-4e58-b44a-0b70beb9edef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14655524-daac-4be8-a4de-d1bb91c00929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2fba498-a157-4722-92a5-4d61ebc2b62f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'groundingdino'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgroundingdino\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model, load_image, predict\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msam2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msam2_image_predictor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAM2ImagePredictor\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgrounding_sam2_demo\u001b[39m(image_path, text_prompt):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'groundingdino'"
     ]
    }
   ],
   "source": [
    "# pip install groundingdino-py\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "def grounding_sam2_demo(image_path, text_prompt):\n",
    "    \"\"\"GroundingDINO + SAM2 по текстовому промпту\"\"\"\n",
    "    \n",
    "    print(f\"🎯 Ищем: '{text_prompt}'\")\n",
    "    \n",
    "    # 1. Загрузка GroundingDINO\n",
    "    print(\"1. Загружаем GroundingDINO...\")\n",
    "    grounding_model = load_model(\n",
    "        \"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \n",
    "        \"weights/groundingdino_swint_ogc.pth\"  # Автоскачается\n",
    "    )\n",
    "    \n",
    "    # 2. Загрузка изображения\n",
    "    image_source, image = load_image(image_path)\n",
    "    \n",
    "    # 3. Детекция по тексту\n",
    "    print(\"2. Детекция по тексту...\")\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=grounding_model,\n",
    "        image=image,\n",
    "        caption=text_prompt,\n",
    "        box_threshold=0.3,    # Порог уверенности bbox\n",
    "        text_threshold=0.25   # Порог соответствия тексту\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Найдено объектов: {len(boxes)}\")\n",
    "    for i, (box, phrase, logit) in enumerate(zip(boxes, phrases, logits)):\n",
    "        print(f\"   {i+1}. {phrase} (уверенность: {logit:.3f})\")\n",
    "    \n",
    "    # 4. Загрузка SAM2\n",
    "    print(\"3. Загружаем SAM2...\")\n",
    "    sam_predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-tiny\")\n",
    "    \n",
    "    # 5. Сегментация найденных объектов\n",
    "    print(\"4. Сегментация...\")\n",
    "    sam_predictor.set_image(image_source)\n",
    "    \n",
    "    all_masks = []\n",
    "    object_info = []\n",
    "    \n",
    "    for i, (box, phrase, logit) in enumerate(zip(boxes, phrases, logits)):\n",
    "        # Конвертируем bbox в формат SAM2\n",
    "        H, W = image_source.shape[1], image_source.shape[2]\n",
    "        x1, y1, x2, y2 = box * torch.tensor([W, H, W, H])\n",
    "        bbox = np.array([x1, y1, x2, y2])\n",
    "        \n",
    "        masks, scores, _ = sam_predictor.predict(box=bbox[None, :])\n",
    "        \n",
    "        all_masks.append(masks[0])\n",
    "        object_info.append({\n",
    "            'mask': masks[0],\n",
    "            'phrase': phrase,\n",
    "            'confidence': logit,\n",
    "            'score': scores[0]\n",
    "        })\n",
    "    \n",
    "    # 6. Визуализация\n",
    "    visualize_grounding_sam2(image_source, object_info, text_prompt)\n",
    "\n",
    "def visualize_grounding_sam2(image, object_info, text_prompt):\n",
    "    \"\"\"Визуализация результатов\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.imshow(image[0].permute(1, 2, 0).cpu().numpy())\n",
    "    \n",
    "    for i, obj in enumerate(object_info):\n",
    "        show_mask(obj['mask'], plt.gca(), random_color=True)\n",
    "        \n",
    "        mask_center = find_mask_center(obj['mask'])\n",
    "        label = f\"{obj['phrase']} ({obj['confidence']:.2f})\"\n",
    "        \n",
    "        plt.text(mask_center[0], mask_center[1], label, \n",
    "                fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                         facecolor='white', \n",
    "                         edgecolor='black', \n",
    "                         alpha=0.8),\n",
    "                ha='center', va='center')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title(f\"GroundingDINO+SAM2: '{text_prompt}'\\n{len(object_info)} объектов\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def find_mask_center(mask):\n",
    "    y_coords, x_coords = np.where(mask)\n",
    "    if len(x_coords) > 0 and len(y_coords) > 0:\n",
    "        center_x = np.mean(x_coords)\n",
    "        center_y = np.mean(y_coords)\n",
    "        return (center_x, center_y)\n",
    "    else:\n",
    "        return (0, 0)\n",
    "\n",
    "# 🎯 Примеры использования:\n",
    "print(\"=== Пример 1: Поиск конкретных объектов ===\")\n",
    "grounding_sam2_demo(\n",
    "    \"/home/oleg/projects/ML/CLIP/images/0d354ad89e92986b19b10a8ac2797dfb.jpg\",\n",
    "    \"cup . laptop\"  # 👈 Найди ТОЛЬКО чашку и ноутбук\n",
    ")\n",
    "\n",
    "print(\"\\n=== Пример 2: Поиск по описанию ===\")\n",
    "grounding_sam2_demo(\n",
    "    \"/home/oleg/projects/ML/CLIP/images/0d354ad89e92986b19b10a8ac2797dfb.jpg\", \n",
    "    \"person . plant\"  # 👈 Найди ТОЛЬКО человека и растение\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb8461-9fe3-416a-a4ca-53a21843c602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensflow",
   "language": "python",
   "name": "tensflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
