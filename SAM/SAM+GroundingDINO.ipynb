{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bdc645-2b43-41d7-9404-35803919e577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8adfa524-1b3b-476b-bd84-f27d6acc6011",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'groundingdino'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgroundingdino\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model, load_image, predict\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msam2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msam2_image_predictor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAM2ImagePredictor\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimple_grounding_sam2\u001b[39m(image_path, text_prompt):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'groundingdino'"
     ]
    }
   ],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∏: pip install groundingdino-py\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "def simple_grounding_sam2(image_path, text_prompt):\n",
    "    \"\"\"–ü—Ä–æ—Å—Ç–æ–π GroundingDINO + SAM2\"\"\"\n",
    "    \n",
    "    print(f\"üîç –ò—â–µ–º: {text_prompt}\")\n",
    "    \n",
    "    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏\n",
    "    print(\"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º GroundingDINO...\")\n",
    "    grounding_model = load_model(\n",
    "        \"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \n",
    "        \"weights/groundingdino_swint_ogc.pth\"\n",
    "    )\n",
    "    \n",
    "    print(\"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º SAM2...\")\n",
    "    sam_model = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-tiny\")\n",
    "    \n",
    "    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "    image_source, image = load_image(image_path)\n",
    "    \n",
    "    # 3. –ò—â–µ–º –æ–±—ä–µ–∫—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É\n",
    "    print(\"üéØ –ò—â–µ–º –æ–±—ä–µ–∫—Ç—ã...\")\n",
    "    boxes, scores, phrases = predict(\n",
    "        model=grounding_model,\n",
    "        image=image,\n",
    "        caption=text_prompt,\n",
    "        box_threshold=0.3,\n",
    "        text_threshold=0.25\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(boxes)} –æ–±—ä–µ–∫—Ç–æ–≤\")\n",
    "    for i, (phrase, score) in enumerate(zip(phrases, scores)):\n",
    "        print(f\"   {i+1}. {phrase} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {score:.3f})\")\n",
    "    \n",
    "    # 4. –°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã\n",
    "    print(\"‚úÇÔ∏è –°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º...\")\n",
    "    sam_model.set_image(image_source)\n",
    "    \n",
    "    all_masks = []\n",
    "    \n",
    "    for i, box in enumerate(boxes):\n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã\n",
    "        H, W = image_source.shape[1], image_source.shape[2]\n",
    "        x1, y1, x2, y2 = box * torch.tensor([W, H, W, H])\n",
    "        bbox = np.array([x1, y1, x2, y2])\n",
    "        \n",
    "        # –°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç\n",
    "        masks, mask_scores, _ = sam_model.predict(box=bbox[None, :])\n",
    "        all_masks.append(masks[0])\n",
    "    \n",
    "    # 5. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    print(\"üñºÔ∏è –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç...\")\n",
    "    \n",
    "    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    image_display = image_source[0].permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image_display)\n",
    "    \n",
    "    # –†–∏—Å—É–µ–º –º–∞—Å–∫–∏\n",
    "    for i, mask in enumerate(all_masks):\n",
    "        # –°–ª—É—á–∞–π–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è –∫–∞–∂–¥–æ–π –º–∞—Å–∫–∏\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        plt.imshow(mask_image)\n",
    "        \n",
    "        # –ü–æ–¥–ø–∏—Å—å –æ–±—ä–µ–∫—Ç–∞\n",
    "        center_y, center_x = np.where(mask)\n",
    "        if len(center_x) > 0 and len(center_y) > 0:\n",
    "            text_x = np.mean(center_x)\n",
    "            text_y = np.mean(center_y)\n",
    "            \n",
    "            plt.text(text_x, text_y, f\"{phrases[i]}\\n({scores[i]:.2f})\", \n",
    "                    fontsize=12, fontweight='bold', color='white',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='black', alpha=0.8),\n",
    "                    ha='center', va='center')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title(f\"–ù–∞–π–¥–µ–Ω–æ: {text_prompt}\\n{len(boxes)} –æ–±—ä–µ–∫—Ç–æ–≤\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üéâ –ì–æ—Ç–æ–≤–æ!\")\n",
    "\n",
    "# üéØ –ü–†–û–°–¢–´–ï –ü–†–ò–ú–ï–†–´:\n",
    "\n",
    "print(\"=== –ü–†–ò–ú–ï–† 1: –õ—é–¥–∏ –∏ —Ç–µ—Ö–Ω–∏–∫–∞ ===\")\n",
    "simple_grounding_sam2(\n",
    "    \"/home/oleg/projects/ML/CLIP/images/0d354ad89e92986b19b10a8ac2797dfb.jpg\",\n",
    "    \"person . laptop . cup\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== –ü–†–ò–ú–ï–† 2: –¢–æ–ª—å–∫–æ —á–µ–ª–æ–≤–µ–∫ ===\")\n",
    "simple_grounding_sam2(\n",
    "    \"/home/oleg/projects/ML/CLIP/images/0d354ad89e92986b19b10a8ac2797dfb.jpg\",\n",
    "    \"person\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== –ü–†–ò–ú–ï–† 3: –†–∞—Å—Ç–µ–Ω–∏—è –∏ –ø–æ—Å—É–¥–∞ ===\")\n",
    "simple_grounding_sam2(\n",
    "    \"/home/oleg/projects/ML/CLIP/images/0d354ad89e92986b19b10a8ac2797dfb.jpg\",\n",
    "    \"plant . cup\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec31fe9-b0ae-4c5c-ba47-6e9b6e038616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729b5a1-c1b4-4e58-b44a-0b70beb9edef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14655524-daac-4be8-a4de-d1bb91c00929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2fba498-a157-4722-92a5-4d61ebc2b62f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'groundingdino'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgroundingdino\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model, load_image, predict\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msam2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msam2_image_predictor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAM2ImagePredictor\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgrounding_sam2_demo\u001b[39m(image_path, text_prompt):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'groundingdino'"
     ]
    }
   ],
   "source": [
    "# pip install groundingdino-py\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "def grounding_sam2_demo(image_path, text_prompt):\n",
    "    \"\"\"GroundingDINO + SAM2 –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –ø—Ä–æ–º–ø—Ç—É\"\"\"\n",
    "    \n",
    "    print(f\"üéØ –ò—â–µ–º: '{text_prompt}'\")\n",
    "    \n",
    "    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ GroundingDINO\n",
    "    print(\"1. –ó–∞–≥—Ä—É–∂–∞–µ–º GroundingDINO...\")\n",
    "    grounding_model = load_model(\n",
    "        \"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \n",
    "        \"weights/groundingdino_swint_ogc.pth\"  # –ê–≤—Ç–æ—Å–∫–∞—á–∞–µ—Ç—Å—è\n",
    "    )\n",
    "    \n",
    "    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    image_source, image = load_image(image_path)\n",
    "    \n",
    "    # 3. –î–µ—Ç–µ–∫—Ü–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É\n",
    "    print(\"2. –î–µ—Ç–µ–∫—Ü–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É...\")\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=grounding_model,\n",
    "        image=image,\n",
    "        caption=text_prompt,\n",
    "        box_threshold=0.3,    # –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ bbox\n",
    "        text_threshold=0.25   # –ü–æ—Ä–æ–≥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–µ–∫—Å—Ç—É\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {len(boxes)}\")\n",
    "    for i, (box, phrase, logit) in enumerate(zip(boxes, phrases, logits)):\n",
    "        print(f\"   {i+1}. {phrase} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {logit:.3f})\")\n",
    "    \n",
    "    # 4. –ó–∞–≥—Ä—É–∑–∫–∞ SAM2\n",
    "    print(\"3. –ó–∞–≥—Ä—É–∂–∞–µ–º SAM2...\")\n",
    "    sam_predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-tiny\")\n",
    "    \n",
    "    # 5. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤\n",
    "    print(\"4. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è...\")\n",
    "    sam_predictor.set_image(image_source)\n",
    "    \n",
    "    all_masks = []\n",
    "    object_info = []\n",
    "    \n",
    "    for i, (box, phrase, logit) in enumerate(zip(boxes, phrases, logits)):\n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º bbox –≤ —Ñ–æ—Ä–º–∞—Ç SAM2\n",
    "        H, W = image_source.shape[1], image_source.shape[2]\n",
    "        x1, y1, x2, y2 = box * torch.tensor([W, H, W, H])\n",
    "        bbox = np.array([x1, y1, x2, y2])\n",
    "        \n",
    "        masks, scores, _ = sam_predictor.predict(box=bbox[None, :])\n",
    "        \n",
    "        all_masks.append(masks[0])\n",
    "        object_info.append({\n",
    "            'mask': masks[0],\n",
    "            'phrase': phrase,\n",
    "            'confidence': logit,\n",
    "            'score': scores[0]\n",
    "        })\n",
    "    \n",
    "    # 6. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    visualize_grounding_sam2(image_source, object_info, text_prompt)\n",
    "\n",
    "def visualize_grounding_sam2(image, object_info, text_prompt):\n",
    "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.imshow(image[0].permute(1, 2, 0).cpu().numpy())\n",
    "    \n",
    "    for i, obj in enumerate(object_info):\n",
    "        show_mask(obj['mask'], plt.gca(), random_color=True)\n",
    "        \n",
    "        mask_center = find_mask_center(obj['mask'])\n",
    "        label = f\"{obj['phrase']} ({obj['confidence']:.2f})\"\n",
    "        \n",
    "        plt.text(mask_center[0], mask_center[1], label, \n",
    "                fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                         facecolor='white', \n",
    "                         edgecolor='black', \n",
    "                         alpha=0.8),\n",
    "                ha='center', va='center')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title(f\"GroundingDINO+SAM2: '{text_prompt}'\\n{len(object_info)} –æ–±—ä–µ–∫—Ç–æ–≤\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def find_mask_center(mask):\n",
    "    y_coords, x_coords = np.where(mask)\n",
    "    if len(x_coords) > 0 and len(y_coords) > 0:\n",
    "        center_x = np.mean(x_coords)\n",
    "        center_y = np.mean(y_coords)\n",
    "        return (center_x, center_y)\n",
    "    else:\n",
    "        return (0, 0)\n",
    "\n",
    "# üéØ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "print(\"=== –ü—Ä–∏–º–µ—Ä 1: –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ ===\")\n",
    "grounding_sam2_demo(\n",
    "    \"/home/oleg/projects/ML/CLIP/images/0d354ad89e92986b19b10a8ac2797dfb.jpg\",\n",
    "    \"cup . laptop\"  # üëà –ù–∞–π–¥–∏ –¢–û–õ–¨–ö–û —á–∞—à–∫—É –∏ –Ω–æ—É—Ç–±—É–∫\n",
    ")\n",
    "\n",
    "print(\"\\n=== –ü—Ä–∏–º–µ—Ä 2: –ü–æ–∏—Å–∫ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é ===\")\n",
    "grounding_sam2_demo(\n",
    "    \"/home/oleg/projects/ML/CLIP/images/0d354ad89e92986b19b10a8ac2797dfb.jpg\", \n",
    "    \"person . plant\"  # üëà –ù–∞–π–¥–∏ –¢–û–õ–¨–ö–û —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ä–∞—Å—Ç–µ–Ω–∏–µ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb8461-9fe3-416a-a4ca-53a21843c602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensflow",
   "language": "python",
   "name": "tensflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
